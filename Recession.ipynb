{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recession Identification and Prediction\n",
    "\n",
    "> Winter is coming, because the Indians are collecting firewood like crazy.\n",
    "\n",
    "Team Members: Zichao Xiong, Yutao Yuan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import os\n",
    "from pandas_datareader.data import DataReader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.ar_model import AutoReg as AR\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# The following packages are written by our team \n",
    "# So pleasure ensure you download them from the github repository before running the code\n",
    "from mlf import mlf_plot\n",
    "from mlf import feature_selection\n",
    "from mlf import param_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Background\n",
    "\n",
    "People want the macro-economy to get better every day. We don't want recessions or depressions, but bad things are often known only after they happen. Thus, an effective early warning method is waiting to be discovered. We hope to anticipate economic crises in advance, which will give us time to adopt governmental or fiscal policies (whether or not they actually work).\n",
    "\n",
    "To accomplish this prediction, scientists usually assume that what has happened in the past will happen again in the future. Thus, historical data will provide us with enough information to make predictions. We will follow this idea by combining economic data and machine learning methods to identify or predict the onset of a recession."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Read Data from FRED\n",
    "\n",
    "We read the following data from [Federal Reserve Economic Data](https://fred.stlouisfed.org/) (Fred). The codes of data are defined as follows:\n",
    "\n",
    "1. `'IPMAN'`: Industrial production of manufacturing;\n",
    "2. `'W875RX1'`: Real personal income;\n",
    "3. `'CMRMTSPL'`: Real manufacturing and trade industries sales;\n",
    "4. `'PAYEMS'`: Total number of employees;\n",
    "5. `'FEDFUNDS'`: Federal funds effective interest rate;\n",
    "6. `'CORESTICKM159SFRBATL'`: Consumer price index less food and energy;\n",
    "7. `'USREC'`: (Target) Binary variable indicating whether America is in a recession or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '1950-01-01'\n",
    "end = '2023-12-01'\n",
    "\n",
    "# X variables\n",
    "# Production related\n",
    "production = DataReader('IPMAN', 'fred', start=start, end=end)\n",
    "income = DataReader('W875RX1', 'fred', start=start, end=end)\n",
    "sales = DataReader('CMRMTSPL', 'fred', start=start, end=end)\n",
    "# Employment\n",
    "employees = DataReader('PAYEMS', 'fred', start=start, end=end)\n",
    "# Money related\n",
    "interest_rate = DataReader('FEDFUNDS', 'fred', start=start, end=end)\n",
    "# Consumption index\n",
    "cpi = DataReader('CORESTICKM159SFRBATL', 'fred', start=start, end=end)\n",
    "\n",
    "# y variables\n",
    "recession = DataReader('USREC', 'fred', start=start, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat all the data into dataframe\n",
    "df_raw = pd.concat((production, income, sales, employees, interest_rate, cpi, recession), axis=1)\n",
    "# Change column names\n",
    "X_columns_raw = ['production', 'income', 'sales', 'employees', 'interest_rate', 'cpi']\n",
    "columns_raw = X_columns_raw + ['recession']\n",
    "df_raw.columns = columns_raw\n",
    "df_raw.index.freq = df_raw.index.inferred_freq\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots of these six X variables will be shown as follows, where the gray regions denote recession periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15,9))\n",
    "\n",
    "mlf_colors = ['#63b2ee', '#76da91', '#f8cb7f', '#f89588', \n",
    "              '#7cd6cf', '#9192ab', '#7898e1', '#efa666', \n",
    "              '#eddd86', '#9987ce', '#63b2ee', '#76da91']\n",
    "\n",
    "for k, column in enumerate(X_columns_raw):\n",
    "    i, j = k//2, k%2\n",
    "    axes[i, j].plot(df_raw.index, df_raw[column], label=column, c = mlf_colors[k])\n",
    "    ylim = axes[i, j].get_ylim()\n",
    "    axes[i, j].fill_between(df_raw.index, ylim[0], ylim[1], df_raw['recession'], facecolor='k', alpha=0.1, label = 'recession ')\n",
    "    axes[i, j].legend(loc = 'upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ungrouped_X_columns = []\n",
    "grouped_X_columns = []\n",
    "for column in X_columns_raw:\n",
    "    foo = []\n",
    "    for n in [1, 3, 6, 12]:\n",
    "        # Name the differentiate variable as 'dln_***_*m'\n",
    "        d_column_name = 'dln_' + column + '_' + str(n) + 'm'\n",
    "        df_raw[d_column_name] = np.log(df_raw[column]) - np.log(df_raw[column].shift(n))\n",
    "        ungrouped_X_columns.append(d_column_name)\n",
    "        foo.append(d_column_name)\n",
    "    grouped_X_columns.append(foo)\n",
    "\n",
    "# Drop all the NaN rows\n",
    "df = df_raw.dropna(axis=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Split Training set and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = df.index[0]\n",
    "train_end = '2005-12-01'\n",
    "\n",
    "test_start = '2006-01-01'\n",
    "test_end = '2023-12-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Hidden Markov Chain\n",
    "\n",
    "The most famous way to analyze recessions is hidden markov chain and factor method. In general, it is assumed that both economic output and recession are determined by a hidden factor. Once we discover the factor through certain model, we can predict recessions using this factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ![Factor Model](graph/factor_model.jpg){height=80px} -->\n",
    "\n",
    "<!-- <img src=\"graph/factor_model.jpg\" alt=\"Factor Model\" style=\"height:10%\"> -->\n",
    "\n",
    "<div>\n",
    "<img src=\"graph/factor_model.jpg\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we define the model as follows\n",
    "$$\n",
    "\\begin{align}\n",
    "x_{i,t} & = \\lambda_i f_t + u_{i,t} \\\\\n",
    "u_{i,t} & = c_{i,1} u_{1,t-1} + c_{i,2} u_{i,t-2} + \\varepsilon_{i,t} \\qquad & \\varepsilon_{i,t} \\sim N(0, \\sigma_i^2) \\\\\n",
    "f_t & = a_1 f_{t-1} + a_2 f_{t-2} + \\eta_t \\qquad & \\eta_t \\sim N(0, I)\\\\\n",
    "y_{t} & = \\text{Logistic}(f_t) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here $x_{i, t}$ are `[production, income, sales, employees, interest_rate, cpi]`. $f_{i, t}$ is the hidden factor and $u_{i, t}$ is the error term. We assume that $f_{i, t}$ and $u_{i, t}$ follows AR(2) process. And recession indicator $y_{i, t}$ is a function of factor $f_{i, t}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_columns_hmm = [column for column in ungrouped_X_columns if '1m' in column]\n",
    "X_hmm = df[X_columns_hmm]\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_hmm_std = sc.fit_transform(X_hmm)\n",
    "X_hmm_std = pd.DataFrame(X_hmm_std, columns = X_columns_hmm, index=df.index)\n",
    "model = sm.tsa.DynamicFactor(X_hmm_std, k_factors=1, factor_order=2, error_order=2)\n",
    "\n",
    "initial_res = model.fit(method='powell', disp=False)\n",
    "res = model.fit(initial_res.params, disp=False)\n",
    "res.summary(separate_params=False).tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that coefficients of $f_{i, t}$ on productions, sales, income and employees are all negative, which means that $f_{i, t}$ is a factor depicting recession to some degree. But the relationship between $f_{i, t}$ and binary $y$ is not so straight forward. We have to use a logistic regression to discover it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = res.factors.filtered[0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,3))\n",
    "ax.plot(df.index, factor, label='factor', c = mlf_colors[0])\n",
    "ylim = ax.get_ylim()\n",
    "ax.fill_between(df.index, ylim[0], ylim[1], df['recession'], facecolor='k', alpha=0.1, label='recession ')\n",
    "ax.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(factor.reshape(-1, 1), df['recession'].to_numpy(), test_size=0.3, random_state=42)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(f'The identification accuracy of factor model is {model.score(X_test, y_test):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of hidden markov chain is not so satisfying. Maybe it can be improved by setting more complicated models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Machine Learning Methods\n",
    "\n",
    "The main topic of this project is to use machine learning methods to identify and predict recessions. As mentioned before, we created a lot of X variables (6 original features times 4 kinds of difference). At this point we are often expected to do feature extraction or feature selection. First let us define the training dataset and testing dataset based on time. Here we define training set as the first valid date to 2005-12, and test set as 2006-01 to 2023-12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[train_start:train_end, ungrouped_X_columns]\n",
    "y_train = df.loc[train_start:train_end, 'recession']\n",
    "\n",
    "X_test = df.loc[test_start:test_end, ungrouped_X_columns]\n",
    "y_test = df.loc[test_start:test_end, 'recession']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 All Feature Used\n",
    "\n",
    "More information is not always best, and we will illustrate this by using all features in the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identification_score_unselected = param_search.multi_search(X_train, y_train, X_test, y_test, feature_type = 'unselected')\n",
    "display(identification_score_unselected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Feature Extraction (PCA)\n",
    "\n",
    "In previous lessons, we learned ways to extract new variables from the original features. The most common one is PCA, but it has two drawbacks:\n",
    "\n",
    "1. The new data has less economic meanings and are harder to understand intuitively;\n",
    "2. This method is unsupervised so it may ignore the y variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "length, width = X_train_std.shape\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(X_train_std)\n",
    "X_train_pca = pca.transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "\n",
    "identification_score_pca = param_search.multi_search(X_train_pca, y_train, X_test_pca, y_test, std = False, feature_type = 'pca')\n",
    "display(identification_score_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Feature Selection (L1 Norm LR Feature Selection)\n",
    "\n",
    "If we prefer to preserve the original data meaning, what we should do is feature selection. A common approach is to select features based on sparsity of L1-norm regression. Here we decide to choose no more than 12 features to identify recessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_feature_selection = feature_selection.LogisticFeatureSelection(X_train, y_train, np.array(ungrouped_X_columns))\n",
    "lr_feature_selection.lr_fit()\n",
    "lr_feature_selection.lr_plot_feature()\n",
    "feature_column_lr = lr_feature_selection.lr_select_feature(less_than = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The selected features are: ', feature_column_lr)\n",
    "\n",
    "X_train_l1_lr = df.loc[train_start:train_end, feature_column_lr]\n",
    "X_test_l1_lr = df.loc[test_start:test_end, feature_column_lr]\n",
    "\n",
    "identification_score_l1_lr = param_search.multi_search(X_train_l1_lr, y_train, X_test_l1_lr, y_test, feature_type = 'l1_lr')\n",
    "display(identification_score_l1_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 Feature Selection (Semi-auto Method)\n",
    "\n",
    "One question for L1 norm LR is why the features selected via LR can be applied to other machine learning methods (e.g. SVM or DT). We also find a large imbalance in the features selected by this method, with too many sales related features selected, but no interest related features.\n",
    "\n",
    "Moro et al. (2014) use a semi-auto feature selection method. It is called semi-automatic because this method requires first grouping features using intuition, and then for each group various machine learning methods can be applied to select features. The process is given as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"graph/semiauto_step_1.jpg\" height=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"graph/semiauto_step_2.jpg\" height=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"graph/semiauto_step_3.jpg\" height=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(penalty='l2', \n",
    "                          C=0.1,\n",
    "                          solver='lbfgs',\n",
    "                          random_state=1)\n",
    "\n",
    "clf2 = SVC(kernel='rbf', \n",
    "           gamma=0.1,\n",
    "           C=10,\n",
    "           random_state=1,\n",
    "           probability=True)\n",
    "\n",
    "clf3 = RandomForestClassifier(n_estimators=100,\n",
    "                              criterion='entropy',\n",
    "                              max_depth=6,\n",
    "                              bootstrap=True,\n",
    "                              random_state=1)\n",
    "\n",
    "classifiers = [clf1, clf2, clf3]\n",
    "\n",
    "sel = feature_selection.SemiautoFeatureSelection(T1 = 0.6, T2 = 0.65, T3 = 0.01, T4 = 0.7, classifiers = classifiers)\n",
    "sel.comp_score(X_train, y_train, X_test, y_test, grouped_X_columns)\n",
    "label = sel.first_select(grouped_X_columns)\n",
    "group_SemiautoFeatureSelection = sel.second_select(X_train, y_train, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_semiauto = df.loc[train_start:train_end]\n",
    "X_test_semiauto = df.loc[test_start:test_end]\n",
    "\n",
    "accuracy_lr, roc_auc_lr = param_search.lr_search(X_train_semiauto[group_SemiautoFeatureSelection[clf1]], y_train, \n",
    "                                                 X_test_semiauto[group_SemiautoFeatureSelection[clf1]], y_test)\n",
    "accuracy_svm, roc_auc_svm = param_search.svm_search(X_train_semiauto[group_SemiautoFeatureSelection[clf2]], y_train, \n",
    "                                                    X_test_semiauto[group_SemiautoFeatureSelection[clf2]], y_test)\n",
    "accuracy_rf, roc_auc_rf = param_search.rf_search(X_train_semiauto[group_SemiautoFeatureSelection[clf3]], y_train, \n",
    "                                                 X_test_semiauto[group_SemiautoFeatureSelection[clf3]], y_test)\n",
    "\n",
    "multi_index = pd.MultiIndex.from_product([['semiauto'], ['accuracy', 'roc_auc']], names=['feature_type', 'score'])\n",
    "index = ['lr','svm','rf']\n",
    "data = np.array([[accuracy_lr, roc_auc_lr], [accuracy_svm, roc_auc_svm], [accuracy_rf, roc_auc_rf]])\n",
    "identification_score_semiauto = pd.DataFrame(data, index = index, columns = multi_index)\n",
    "display(identification_score_semiauto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identification_score = pd.concat((identification_score_unselected, identification_score_l1_lr, identification_score_semiauto), axis=1)\n",
    "display(identification_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identification_score = pd.concat((identification_score_pca, identification_score_unselected, identification_score_l1_lr, identification_score_semiauto), axis=1)\n",
    "display(identification_score)\n",
    "\n",
    "labels = ['LR', 'SVM', 'RF']\n",
    "x = np.arange(len(labels))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects0 = ax.bar(x - 1.5*bar_width, identification_score[('unselected', 'accuracy')], bar_width, color = mlf_colors[0], label='Unselected')\n",
    "rects1 = ax.bar(x - 0.5*bar_width, identification_score[('pca', 'accuracy')], bar_width, color = mlf_colors[1], label='PCA')\n",
    "rects2 = ax.bar(x + 0.5*bar_width, identification_score[('l1_lr', 'accuracy')], bar_width, color = mlf_colors[2], label='L1_LR')\n",
    "rects3 = ax.bar(x + 1.5*bar_width, identification_score[('semiauto', 'accuracy')], bar_width, color = mlf_colors[3], label='Semi-auto')\n",
    "\n",
    "ax.set_ylim((0.8, 1.0))\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "labels = ['LR', 'SVM', 'RF']\n",
    "x = np.arange(len(labels))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects0 = ax.bar(x - 1.5*bar_width, identification_score[('unselected', 'roc_auc')], bar_width, color = mlf_colors[0], label='Unselected')\n",
    "rects1 = ax.bar(x - 0.5*bar_width, identification_score[('pca', 'roc_auc')], bar_width, color = mlf_colors[1], label='PCA')\n",
    "rects2 = ax.bar(x + 0.5*bar_width, identification_score[('l1_lr', 'roc_auc')], bar_width, color = mlf_colors[2], label='L1_LR')\n",
    "rects3 = ax.bar(x + 1.5*bar_width, identification_score[('semiauto', 'roc_auc')], bar_width, color = mlf_colors[3], label='Semi-auto')\n",
    "\n",
    "ax.set_ylim((0.8, 1.0))\n",
    "ax.set_ylabel('ROC AUC')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prediction\n",
    "\n",
    "Note that in the previous chapter, we accomplish just the identification, i.e., using the independent variable for a given month to compute the dependent variable for the same month. This may yield good results, but it is not very practical, due to two reasons:\n",
    "\n",
    "1. When we get this month's economic data, it is also determined that a recession is happening or not;\n",
    "2. To make matters worse, some economic data is less current, which means that we can't access the current month's data until about two months later, and then the act of identification becomes just a historiographical study.\n",
    "\n",
    "So we are more curious about whether machine learning algorithms can predict recessions. We will define the following variable: `'recession_in_6m'`. This variable represents whether, for any month $t$, a recession occurred from month $t$ to $t+5$. If so, then `'recession_in_6m'` will take the value of 1 and otherwise 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = df.copy()\n",
    "foo = df_pred.sort_index(ascending=False)\n",
    "foo.loc[:, 'recession_in_6m'] = foo.loc[:, 'recession'].rolling(6, min_periods=1).max().astype('int')\n",
    "df_pred = foo.sort_index(ascending=True)\n",
    "df_pred.drop(columns='recession', inplace=True)\n",
    "df_pred.dropna(inplace=True)\n",
    "\n",
    "X_train_pred = df_pred.loc[train_start:train_end, ungrouped_X_columns]\n",
    "y_train_pred = df_pred.loc[train_start:train_end, 'recession_in_6m']\n",
    "\n",
    "X_test_pred = df_pred.loc[test_start:test_end, ungrouped_X_columns]\n",
    "y_test_pred = df_pred.loc[test_start:test_end, 'recession_in_6m']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have done before, we will predict recessions by 4 steps:\n",
    "\n",
    "1. Use all features;\n",
    "2. Extract features by PCA;\n",
    "3. Use L1-LR to select features;\n",
    "4. Use semi-auto method to select feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 All Features Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_score_unselected = param_search.multi_search(X_train_pred, y_train_pred, X_test_pred, y_test_pred, feature_type = 'unselected')\n",
    "display(prediction_score_unselected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature Extraction (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pred_std = sc.fit_transform(X_train_pred)\n",
    "X_test_pred_std = sc.transform(X_test_pred)\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(X_train_pred_std)\n",
    "X_train_pred_pca = pca.transform(X_train_pred_std)\n",
    "X_test_pred_pca = pca.transform(X_test_pred_std)\n",
    "\n",
    "prediction_score_pca = param_search.multi_search(X_train_pred_pca, y_train_pred, X_test_pred_pca, y_test_pred, std = False, feature_type = 'pca')\n",
    "display(prediction_score_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Feature Selection (L1 Norm LR Feature Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_feature_selection = feature_selection.LogisticFeatureSelection(X_train_pred, y_train_pred, np.array(ungrouped_X_columns))\n",
    "lr_feature_selection.lr_fit()\n",
    "lr_feature_selection.lr_plot_feature()\n",
    "feature_column_lr = lr_feature_selection.lr_select_feature(less_than = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The selected features are: ', feature_column_lr)\n",
    "\n",
    "X_train_pred_l1_lr = df_pred.loc[train_start:train_end, feature_column_lr]\n",
    "X_test_pred_l1_lr = df_pred.loc[test_start:test_end, feature_column_lr]\n",
    "\n",
    "prediction_score_l1_lr = param_search.multi_search(X_train_pred_l1_lr, y_train_pred, X_test_pred_l1_lr, y_test_pred, feature_type = 'l1_lr')\n",
    "display(prediction_score_l1_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Feature Selection (Semi-auto Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(penalty='l2', \n",
    "                          C=0.1,\n",
    "                          solver='lbfgs',\n",
    "                          random_state=1)\n",
    "\n",
    "clf2 = SVC(kernel='rbf', \n",
    "           gamma=0.1,\n",
    "           C=10,\n",
    "           random_state=1,\n",
    "           probability=True)\n",
    "\n",
    "clf3 = RandomForestClassifier(n_estimators=100,\n",
    "                              criterion='entropy',\n",
    "                              max_depth=6,\n",
    "                              bootstrap=True,\n",
    "                              random_state=1)\n",
    "\n",
    "classifiers = [clf1, clf2, clf3]\n",
    "\n",
    "sel = feature_selection.SemiautoFeatureSelection(T1 = 0.6, T2 = 0.65, T3 = 0.01, T4 = 0.7, classifiers = classifiers)\n",
    "sel.comp_score(X_train_pred, y_train_pred, X_test_pred, y_test_pred, grouped_X_columns)\n",
    "label = sel.first_select(grouped_X_columns)\n",
    "group_SemiautoFeatureSelection = sel.second_select(X_train_pred, y_train_pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pred_semiauto = df_pred.loc[train_start:train_end]\n",
    "X_test_pred_semiauto = df_pred.loc[test_start:test_end]\n",
    "\n",
    "accuracy_lr, roc_auc_lr = param_search.lr_search(X_train_pred_semiauto[group_SemiautoFeatureSelection[clf1]], y_train_pred, \n",
    "                                                 X_test_pred_semiauto[group_SemiautoFeatureSelection[clf1]], y_test_pred)\n",
    "accuracy_svm, roc_auc_svm = param_search.svm_search(X_train_pred_semiauto[group_SemiautoFeatureSelection[clf2]], y_train_pred, \n",
    "                                                    X_test_pred_semiauto[group_SemiautoFeatureSelection[clf2]], y_test_pred)\n",
    "accuracy_rf, roc_auc_rf = param_search.rf_search(X_train_pred_semiauto[group_SemiautoFeatureSelection[clf3]], y_train_pred, \n",
    "                                                 X_test_pred_semiauto[group_SemiautoFeatureSelection[clf3]], y_test_pred)\n",
    "\n",
    "multi_index = pd.MultiIndex.from_product([['semiauto'], ['accuracy', 'roc_auc']], names=['feature_type', 'score'])\n",
    "index = ['lr','svm','rf']\n",
    "data = np.array([[accuracy_lr, roc_auc_lr], [accuracy_svm, roc_auc_svm], [accuracy_rf, roc_auc_rf]])\n",
    "prediction_score_semiauto = pd.DataFrame(data, index = index, columns = multi_index)\n",
    "display(prediction_score_semiauto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_score = pd.concat((prediction_score_pca, prediction_score_unselected, prediction_score_l1_lr, prediction_score_semiauto), axis=1)\n",
    "display(prediction_score)\n",
    "\n",
    "labels = ['LR', 'SVM', 'RF']\n",
    "x = np.arange(len(labels))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects0 = ax.bar(x - 1.5*bar_width, prediction_score[('unselected', 'accuracy')], bar_width, color = mlf_colors[0], label='Unselected')\n",
    "rects1 = ax.bar(x - 0.5*bar_width, prediction_score[('pca', 'accuracy')], bar_width, color = mlf_colors[1], label='PCA')\n",
    "rects2 = ax.bar(x + 0.5*bar_width, prediction_score[('l1_lr', 'accuracy')], bar_width, color = mlf_colors[2], label='L1_LR')\n",
    "rects3 = ax.bar(x + 1.5*bar_width, prediction_score[('semiauto', 'accuracy')], bar_width, color = mlf_colors[3], label='Semi-auto')\n",
    "\n",
    "ax.set_ylim((0.6, 1.0))\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "labels = ['LR', 'SVM', 'RF']\n",
    "x = np.arange(len(labels))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects0 = ax.bar(x - 1.5*bar_width, prediction_score[('unselected', 'roc_auc')], bar_width, color = mlf_colors[0], label='Unselected')\n",
    "rects1 = ax.bar(x - 0.5*bar_width, prediction_score[('pca', 'roc_auc')], bar_width, color = mlf_colors[1], label='PCA')\n",
    "rects2 = ax.bar(x + 0.5*bar_width, prediction_score[('l1_lr', 'roc_auc')], bar_width, color = mlf_colors[2], label='L1_LR')\n",
    "rects3 = ax.bar(x + 1.5*bar_width, prediction_score[('semiauto', 'roc_auc')], bar_width, color = mlf_colors[3], label='Semi-auto')\n",
    "\n",
    "ax.set_ylim((0.6, 1.0))\n",
    "ax.set_ylabel('ROC AUC')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CNN\n",
    "\n",
    "Remember the image we draw in Chapter 1? As humans, when we see unusual fluctuations in the economy, we predict that a recession is going to happen. Such judgments are made intuitively, without the help of any numbers or calculations. We wonder if machine learning, or more specifically, neural networks, have the same ability, where we feed in images of plotted economic data, and it can look at the images and output a judgment about the recession.\n",
    "\n",
    "To examine this, we first draw the economic figures. Here we use only the raw data and do not use any differentiated results. Similarly, we do not include the axis ticks in plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cnn = df_pred[X_columns_raw+['recession_in_6m']]\n",
    "df_cnn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The following plotting code is deliberately commented as we have already plotted the figures and saved them as local files. If you wish to re-draw the figures, or change the parameters of the plotting, uncomment and re-run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_dpi = 192\n",
    "# length = df_cnn.shape[0]\n",
    "\n",
    "# for n in range(12, length - 6):\n",
    "#     m = n - 12\n",
    "#     fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(800/my_dpi, 800/my_dpi), dpi=my_dpi)\n",
    "#     for k, column in enumerate(X_columns_raw):\n",
    "#         i, j = k//2, k%2\n",
    "#         axes[i, j].plot(df_cnn.index[m:n], df_cnn.iloc[m:n][column], c = mlf_colors[k])\n",
    "#         axes[i, j].set_facecolor(\"white\")\n",
    "#         axes[i, j].set_xticks([])\n",
    "#         axes[i, j].set_yticks([])\n",
    "#         axes[i, j].spines['top'].set_visible(False)\n",
    "#         axes[i, j].spines['right'].set_visible(False)\n",
    "#         axes[i, j].spines['bottom'].set_visible(False)\n",
    "#         axes[i, j].spines['left'].set_visible(False)\n",
    "\n",
    "#     figure_name = df_cnn.index[n].strftime('%Y%m%d') + '-' + str(int(df_cnn.iloc[n]['recession_in_6m']))\n",
    "#     # figure_path = 'figures/' + figure_name + '.png'\n",
    "#     # figure_path = 'figures_nonframe/' + figure_name + '.png'\n",
    "#     figure_path = 'figures_colorful/' + figure_name + '.png'\n",
    "#     plt.savefig(figure_path)\n",
    "#     plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgdir_path=pathlib.Path('figures_nonframe')\n",
    "\n",
    "file_list = sorted([str(path) for path in imgdir_path.glob('*.png')])\n",
    "\n",
    "labels=[int(os.path.basename(file)[-5]) for file in file_list]\n",
    "ds_files_labels = tf.data.Dataset.from_tensor_slices(\n",
    "    (file_list, labels))\n",
    "\n",
    "img_width, img_height = 300,200\n",
    "\n",
    "def load_process(ds_files_labels):\n",
    "    images=[]\n",
    "    labels=[]\n",
    "\n",
    "    for path, label in ds_files_labels:\n",
    "        image = tf.io.read_file(path)\n",
    "        image = tf.image.decode_image(image, channels=3)\n",
    "      # image=tf.image.convert_image_dtype(image, tf.float32)\n",
    "        image = tf.image.resize(image, [img_height, img_width])\n",
    "        image /= 255.0\n",
    "\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "\n",
    "    return tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "\n",
    "ds_images_labels=load_process(ds_files_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_images_labels.batch(18)\n",
    "images, labels = next(iter(ds_train))\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "for i,(image,label) in enumerate(zip(images, labels)):\n",
    "    ax = fig.add_subplot(3, 6, i+1)\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.imshow(image)\n",
    "    ax.set_title('{}'.format(label), size=15)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_images_labels.shuffle(buffer_size=100,\n",
    "                                  reshuffle_each_iteration=False)\n",
    "\n",
    "mnist_train_all = ds_train.take(500)\n",
    "mnist_valid = mnist_train_all.skip(400).batch(40)\n",
    "mnist_train = mnist_train_all.take(100).batch(40)\n",
    "mnist_test = ds_train.skip(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(\n",
    "    filters=32, kernel_size=(5, 5),\n",
    "    strides=(1, 1), padding='same',\n",
    "    data_format='channels_last',\n",
    "    name='conv_1', activation='relu'))\n",
    "\n",
    "model.add(tf.keras.layers.MaxPool2D(\n",
    "    pool_size=(2, 2), name='pool_1'))\n",
    "    \n",
    "model.add(tf.keras.layers.Conv2D(\n",
    "    filters=64, kernel_size=(5, 5),\n",
    "    strides=(1, 1), padding='same',\n",
    "    name='conv_2', activation='relu'))\n",
    "\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), name='pool_2'))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    units=1024, name='fc_1', \n",
    "    activation='relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dropout(\n",
    "    rate=0.5))\n",
    "    \n",
    "model.add(tf.keras.layers.Dense(\n",
    "    units=2, name='fc_2',\n",
    "    activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy']) # same as `tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')`\n",
    "\n",
    "history = model.fit(mnist_train, epochs=10, \n",
    "                    validation_data=mnist_valid, \n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=len(mnist_test)\n",
    "\n",
    "test_results = model.evaluate(mnist_test.batch(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_test=next(iter(mnist_test.batch(94)))\n",
    "\n",
    "preds = model(batch_test[0])\n",
    "preds = tf.argmax(preds, axis=1)\n",
    "\n",
    "images, labels = next(iter(mnist_test.batch(94)))\n",
    "\n",
    "y_pred=preds.numpy()\n",
    "y_test=labels.numpy()\n",
    "\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2.5, 2.5))\n",
    "ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(confmat.shape[0]):\n",
    "    for j in range(confmat.shape[1]):\n",
    "        ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, we can plot the key point to predict recessions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1 = model.get_layer('conv_1')#名字为conv_1的层,有32个filter，每个filter对应一个特征图\n",
    "\n",
    "feature_model = tf.keras.Model(inputs=model.inputs, outputs=layer_1.output)\n",
    "\n",
    "features = feature_model(batch_test[0])\n",
    "\n",
    "num_features = features.shape[-1]\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "# for i in range(num_features):\n",
    "#     # 获取第 i 个特征图\n",
    "#     feature_map = features[1, :, :, i]\n",
    "#     # 显示特征图\n",
    "#     plt.subplot(num_features//8 + 1, 8, i+1)\n",
    "#     plt.imshow(feature_map, cmap='gray')\n",
    "#     plt.axis('off')\n",
    "#上述循环输出的是第二张图片的所有特征图\n",
    "\n",
    "\n",
    "plt.imshow(features[1,:,:,9])#1代表第二张图片，9代表第10个特征图\n",
    "\n",
    "plt.show()\n",
    "# mnist_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "In this project, we use a variety of methods to identify and predict recessions. Following the traditional Hidden Markov Chain Factor Model approach, the accuracy is less than 90%. Meanwhile, the machine learning approach can achieve an accuracy and ROC AUC of over 90% by using economic data within one year. The same approach also performs well in predicting whether or not a recession will occur in the next six months, with an accuracy of around 85%. In particular, semi-auto feature selection performs well in different scenarios.\n",
    "\n",
    "We are more interested in whether machines can tell if a recession is going to occur just by looking at an figure, just as a human could. We make a simple neural network model and train it to recognize economic figures and observe an accuracy of 0.86, which suggests that there may actually be some pattern of the economic crisis.\n",
    "\n",
    "Such a project could be further developed in the future. Hyper-parameters can be chosen more carefully (i.e., using 18 months data to predict recession in 3 months) to get better scores. Also it might be possible to build more complex neural network models that can recognize recession patterns by inputting single variable economics plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Moro, S., Cortez, P., & Rita, P. (2014). A data-driven approach to predict the success of bank telemarketing. *Decision Support Systems*, *62*, 22-31."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Joke about Weather\n",
    "\n",
    "We include this joke because we think doing regressions on economic data is very similar with this joke.\n",
    "\n",
    "> Fall was upon a remote reservation when the Indian tribe asked their new Chief what the coming winter was going to be like. The modern day Chief had never been taught the secrets of the ancients. When he looked at the sky he couldn't tell what the winter was going to be like.\n",
    "> \n",
    "> Better safe than sorry, he said to himself and told his tribe that the winter was indeed expected to be cold and that the members of the village should stock up on firewood to be prepared.\n",
    "> \n",
    "> After several days, our modern Chief got an idea. He went to the phone booth, called the National Weather Service and asked, \"Is the coming winter going to be cold?\"\n",
    "> \n",
    ">\"It looks like this winter is going to be quite cold,\" the meteorologist at the weather service responded.\n",
    ">\n",
    "> So the Chief went back to his people and told them to collect even more firewood in order to be prepared. A week later he called the National Weather Service again. \"Does it still look like it is going to be a very cold winter?\"\n",
    "> \n",
    "> \"Yes,\" the man at National Weather Service again replied, \"It's going to be a very cold winter.\"\n",
    "> \n",
    "> The Chief again went back to his people and ordered them to collect every scrap of firewood they could find. Two weeks later the Chief called the National Weather Service again. \"Are you absolutely sure that the winter is going to be very cold?\"\n",
    "> \n",
    "> \"Absolutely,\" the man replied. \"It's looking more and more like it is going to be one of the coldest winters ever.\"\n",
    "> \n",
    "> \"How can you be so sure?\" the Chief asked.\n",
    "> \n",
    "> The weatherman replied, \"The Indians are collecting firewood like crazy.\"\n",
    ">\n",
    "> 印第安人来问他们的酋长，“今年冬天冷不冷？”酋长也吃不准，但也不好直说不知道，就说：“肯定很冷，大家要多准备过冬用的劈柴。”于是大家就都去准备劈柴。 \n",
    "> \n",
    "> 酋长是个认真负责的人。一个星期之后，他跑到电话亭里打电话给国家气象服务中心，问：“今年冬天冷不冷？”气象中心的人说：“冷得很。”\n",
    "> \n",
    "> 酋长这才放心，回来后又通知了他的子民一遍：“要多准备柴火过冬。”又过了一星期，酋长有点不放心，又给气象中心打电话，被告知：“非常非常冷。”\n",
    "> \n",
    "> 酋长再次通知子民加大准备柴火的力度。两个星期后，酋长又打电话，被告知：“非常非常非常冷。”酋长急忙通知子民要把收集柴火当头等大事来抓，尽一切努力收集柴火。\n",
    "> \n",
    "> 两个星期以后，酋长再次打电话，气象中心的人用极其肯定的语气说：“可以肯定地说，今年的冬天将是有史以来最冷的冬天，因为我们看见印第安人正疯狂地收集柴火。“"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
